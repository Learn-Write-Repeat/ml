<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Classification Problem Evaluation Metrics</title>
    <meta name="theme-color" content="rgb(255,255,255)">
    <meta name="twitter:title" content="Classification Problem Evaluation Metrics">
    <meta name="twitter:description" content="The evaluation metrics compares how accurately the predicted classes match the actual classes for classification problems. ">
    <meta property="og:type" content="article">
    <meta property="og:image" content="">
    <meta name="twitter:card" content="summary">
    <meta name="description" content="The evaluation metrics compares how accurately the predicted classes match the actual classes for classification problems. ">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.4.1/css/bootstrap.min.css">
    <link rel="manifest" href="manifest.json">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bitter">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Crete+Round">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="assets/fonts/fontawesome5-overrides.min.css">
    <link rel="stylesheet" href="assets/css/styles.min.css">
</head>

<body>
    <nav class="navbar navbar-light navbar-expand-md sticky-top" id="navigation" style="padding: 25px 0px;background-color: white;">
        <div class="container-fluid container"><a class="navbar-brand" href="#" style="background-image: url(&quot;assets/img/CODES.gif&quot;);background-position: center;background-size: cover;background-repeat: no-repeat;width: 170px;height: 65px;"></a><button data-toggle="collapse" class="navbar-toggler"
                data-target="#navcol-1"><span class="sr-only">Toggle navigation</span><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse text-uppercase" id="navcol-1" style="font-family: Roboto, sans-serif;font-weight: bold;">
                <ul class="nav navbar-nav ml-auto" style="font-size: 11px;">
                  <li class="nav-item" role="presentation"><a class="nav-link active"
                        href="https://devincept.codes/contribute.html">Contribute</a></li>
                  <li class="nav-item" role="presentation"><a class="nav-link active" href="https://devincept.tech/pricing.html">Free
                        courses</a></li>
                  <li class="nav-item" role="presentation"><a class="nav-link" href="https://devincept.codes/sponsor.html">Sponsor us</a>
                  </li>
                </ul>
            </div>
        </div>
    </nav>
    <section id="hero" style="background-color: #2f1b1b;color: rgb(255,255,255);">
        <div style="padding: 30px;">
            <p class="d-flex justify-content-center" style="font-family: Belgrano, serif;">&nbsp;Machine Learning</p>
            <h1 class="d-flex justify-content-center topic-title" style="font-family: Belgrano, serif;">Classification Problem Evaluation Metrics<br></h1>
        </div>
        <div id="base" style="background-color: rgba(255,255,255,0.1);padding: 10px 0;font-family: Belgrano, serif;">
            <p class="justify-content-center"><i class="fa fa-align-center"></i>&nbsp;&nbsp;<span id="visits">0</span>&nbsp;Reads</p>
        </div>
    </section>
    <section id="sidetab" style="margin-bottom: 0px;">
        <div class="container-fluid">
            <div class="row content-row">
                <div class="col-3 author">
                    <div class="author-details"><div>
    <div class="author">
        
        <h2 class="author-title"> Author's Details </h2>
        
        <h3 class="author's-name"> Drishti Sabhaya </h3>
      
        
        <ul>
            
            <li><a target="_blank" href="https://github.com/DrishtiSabhaya"><i class="fab fa-github"></i></a></li>
            <li><a target="_blank" href="https://www.linkedin.com/in/drishti-sabhaya-83446a193/"><i class="fab fa-linkedin"></i></a></li>
        </ul>
    </div>
</div></div>
                </div>
                <div class="col-9 main-content">
                    <div class="content-inner"><div>
  <div class="blog-content">
        
 
      <h1 id="classification-problem-evaluation-metrics">Classification Problem Evaluation Metrics</h1>
      <p>When building a classification model, in order to check how efficiently the model performs we can evaluate it with
         metrics . The evaluation metrics compares how accurately the predicted classes match the actual classes for
         classification problems.
         As we know that there are several types of datasets and classification can also be binary or multiclass, based on
         that different evaluation matrix which are listed below can be applied in different scenarios.</p>
      <ol>
         <li><strong>Confusion Matrix</strong> </li>
      </ol>
      <p>It can be generally used while dealing with multiclass classification. It is not like any other type of metrics in
         which we will get an integer to decide
         how efficient the model is, rather we deal with four set of values which are True Positive (TP), True Negative (TN),
         False Positive (FP), False Negative (FN).</p>
      <table>
         <tbody>
            <tr>
               <td>&nbsp;</td>
               <td>Positive</td>
               <td>Negative</td>
            </tr>
            <tr>
               <td>Positive</td>
               <td>True Positive (TP) </td>
               <td>False Positive (FP) </td>
            </tr>
            <tr>
               <td>Neagtive</td>
               <td>False Negative (FN) </td>
               <td>True Neagtive (TN) </td>
            </tr>
         </tbody>
      </table>
      <p> Let us take an example so as to not confuse. </p>
      
    <p>  Consider a dataset of pregnancy tests of a woman in which there are two classes : pregnant or not pregnant
      
       TP - It indicates that the woman is actually pregnant and the model also predicts it
      correctly.<br>
       FP - It indicates that the woman is actually not pregnant but the model predicts as pregnant.<br>
       FN - It indicates that the woman is acutally pregnant but the model predicts as not pregnant.<br>
       TN - It indicates that the woman is not pregnant and the model predicts it correctly as not
      pregnant.<br>
      
      Sometimes the cost of FN can be higher and sometimes the cost of FP can be higher.<br>
      </p>
      
      <img src="https://miro.medium.com/max/1400/1*JJ_AEptV8jF7bu17zuVxLg.png" alt="Image"> <br><br>
    <p>  Imagine that if we predict COVID-19 residents as healthy patients and they do not need to quarantine, there would be a
      massive number of COVID-19 infections. Hence the cost of false negatives is much higher than the cost of false
      positives. <br><br></p>
      <img src="https://miro.medium.com/max/1400/1*uLbVblrwaqf1-sVT5A4TRg.png" alt="Image"> <br>
  <p>    Well, since missing important emails will clearly be more of a problem than receiving spam, we can say that in this
      case, FP will have a higher cost than FN.<br>
      Since FN and FP leads to false predictions we need to minimize it. </p>
      
       <strong>2. Accuracy </strong>
      
  <p>    It is one of the most simple and common classification metrics. It is determined by the total number of correct
      predictions divided by total number of predictions made for a dataset. <br>
      <pre>            
      Accuracy =   (total number of correct predictions) / (total number of predictions)   <br>                </pre>
    <p>  In terms of confusion matrix, accuracy can be defined as</p>
      <pre>                  
      Accuracy =  (TP + TN ) / (TP + FP + FN + TN)
                         </pre>
     <p> But this metric has a disadvantage which is it cannot be used for imbalanced class datasets.</p></p>
      
    <p>  <strong>Note :</strong> Balanced class datasets are those in which classes are equally distributed whereas in
      imbalanced dataset there is always a majority present of a particular class.
      <br>For example, if fraud detection dataset consists of 95% of non-fraud classes and only 5% of fraud classes then it is
      called an imbalanced dataset.
      In this case the classification model will be showing 95% accuracy as it trained with the majority non-fraud class only.
      And when given a fraud case to predict, it will predict it as non-fraud class based on training.
      <br> And hence accuracy sometimes can be misleading in a way which would increase our hopes about model or also it may
      cost life of a person if predicted for a particular disease with imbalanced dataset.<br>
      </p>
     <p>3. <strong>Precision</strong>
      
   <p>  In terms of confusion matrix, precision is defined as the ratio of True Positives to all the positives predicted by the
      model.</p>
      
      <pre>                  
      Precision = TP / (TP + FP)
                       </pre>
   <p>   The more False Positive the model predicts, the lower is it&#39;s precision value.</p></p>
      
  <p>   4. <strong>Recall (Senstivity)</strong>
      
      This is defined as the ratio of True Positives to all the positives in your dataset.
      <pre>             
      Recall = TP / (TP + FN)
                   </pre>
    <p> The more False Negatives the model predicts the lower the recall. <br>
      
      The idea of recall and precision seems to be abstract. Let me illustrate the difference with the help of an example.
      
      Let&#39;s say that we have dataset of bank loans with the confusion matrix given below<br></p>
      
      </p>
      <table >
         <tbody  >
            <tr >
               <td>&nbsp;</td>
               <td>Positive </td> 
               <td>Negative </td>
            </tr>
            <tr>
               <td>Positive: </td>
               <td> TP-599 </td>
               <td>  FP-0 </td>
            </tr>
            <tr>
               <td>Negative: </td>
               <td>  FN-33 </td>
               <td> TN-22 </td>
            </tr>
         </tbody>
      </table>
      
      <p>Precision : Out of the loan that is predicted aas a bad loan, how many did we classify correctly:question: </p>
      <pre>                
      Precision = 599 /(599 + 0) = 100%
                       </pre>
      
      <p>Recall : Out of the actual bad loan, how many did we correctly predict as bad loan:question:</p>
      <pre>              
      Recall = 599 / (599 + 33) =  94.5%
                    </pre>
      
      <h4 id="precision-vs-recall">Precision vs Recall</h4>
      <p><img src="https://miro.medium.com/max/700/0*uhuG2rhX6XzNC43X.png" alt="Image"> <br> </p>
      <p>For a particular model, instead of using either precision or recall we can combine both of them to obtain
         <strong>F1-score</strong>.</p>
      <ol>
         <li><strong>F1-score</strong></li>
      </ol>
      <p>For some models, sometimes the recall would be high and sometimes precision would be high. We cannot even take mean
         of these two values as that also would be insignificant measure for evaluating model. Hence we use harmonic mean of
         <strong>precision and recall</strong> which is termed as F1-score. It balances out the both the precision and recall
         in one integer. An F1 score is considered perfect when it’s 1, while the model is a total failure when it’s 0.</p>
      <pre>                                                
      F1-score = 2 * 1 / (1/precision + 1/recall)=   (2 * precision * recall)/(precision + recall)
                                 
      
      </pre>
<p>      In the above example of bank loans, F1-score will be</p>
      <pre>                
      F1-score = (2 * 1 * 0.945) / (1 + 0.945) = 0.971
      
      </pre>
      
      <ol>
         <li><strong>Specificity</strong></li>
      </ol>
      <p>It is also derived from confusion matrix which defines the ability of a model to predict correctly the True Negative
         cases. </p>
      <pre>                
   <p>   Specificity =TN /(TN + FP)   </pre>
 <p>    For example, in cancer detection test specificity will be defined as the person not having cancer and the model
      correctly rejecting the patients who test negative.
     <br> 
      <strong>7. Loss</strong>
      
      It can be used for multiclass classification where it assigns a probability to each class for all examples. It is used
      to compare models not only on the basis of outputs but their probabilistic outcomes.
      
      
      <img src="https://miro.medium.com/max/344/0*zneWdU4GN_WBzTIF.gif" alt="">
      
      <strong>y_ij</strong>, indicates whether sample i belongs to class j or not <br>
      <strong>p_ij</strong>, indicates the probability of sample i belonging to class j
      
     
      <img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/log-loss-curve-768x384.png" alt="" srcset="">
      <br>
      In this graph  we can see that as the probability of the model increases, the logarithmic
      value decreases and vice versa. Hence lower the log value higher accurate will be the model.</p>
      
   <p>  8. <strong>Area Under Curve</strong>
      
      It is used for binary classification. In order to understand area under curve we will first define two terms.
      
      True Positive Rate (Sensitivity) : It is defined as the ratio of True Positive to all the other positives in dataset.</p>
      <pre>           
      TPR = TP/   (TP + FN)  
      </pre>
      </p>
      <p>False Positive Rate : It is defined as the ratio of False Positive to all negatives in dataset. </p>
      <p>
      <pre>    
      FPR =FP/ (FP + TN)<br></pre>
      <img src="https://miro.medium.com/max/345/1*bGf43h_VZ7m7FQAAgtwusw.png" alt="Image"> <br></p>
      <p>The probabilistic interpretation of AUC score is that if you randomly choose a positive case and a negative case, the
         probability that the positive case outranks the negative case according to the classifier is given by the AUC. It is
         defined as the area under the curve which is plotted by FPR vs TPR. Basically, for every threshold, we calculate TPR
         and FPR and plot it on one chart. The value can range from 0 to 1. However AUC score of a random classifier for
         balanced data is 0.5.</p>
      <p>So basically we have 8 <strong>evaluation metrics</strong> for dealing with classification problems and we can
         choose any of them depending on how our dataset and classification model is.</p>

  
 <p class="ipynb-link">You can learn more about this topic from<a href="https://github.com/Learn-Write-Repeat/ml/blob/main/Classification%20Problem%20And%20Evaluation/Drishti_ML_ClassificationProblemEvaluation.ipynb">here</a>.</p>
 
   </div>


</div></div>
                </div>
            </div>
        </div>
    </section>
    <footer class="d-flex flex-column justify-content-center align-items-center" id="footer" style="padding-bottom: 0;">
        <h1>DevIncept.codes</h1>
        <h5>Contact us:</h5>
        <p>Email:<a href="#">&nbsp;support@devincept.tech</a></p>
        <div class="d-flex flex-row" id="social-button"><button class="btn btn-primary" type="button"><a href="https://www.linkedin.com/company/devincept/" target="_blank"><i class="fa fa-linkedin-square" style="font-size: 24px;" href=""></i></a></button><button class="btn btn-primary" type="button"><a href="https://www.facebook.com/DevIncept/" target="_blank"><i class="fa fa-facebook" style="font-size: 24px;"></i></a></button>
            <button
                class="btn btn-primary" type="button"><a href="https://www.instagram.com/devincept.tech/?hl=en" target="_blank"><i class="fa fa-instagram" style="font-size: 24px;"></i></a></button><button class="btn btn-primary" type="button"><a href="https://github.com/Learn-Write-Repeat" target="_blank"><i class="fa fa-github" style="font-size: 24px;"></i></a></button></div>
    </footer>
    <script>
      function cb(response) {
         document.getElementById('visits').innerText = response.value;
      }
   </script>
   <script async src="https://api.countapi.xyz/hit/ml-classification-evaluation-metric/visits?callback=cb"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
</body>

</html>