<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Evaluation Metric</title>
    <meta name="theme-color" content="rgb(255,255,255)">
    <meta name="twitter:title" content="Evaluation Metrics">
    <meta name="twitter:description" content="Evaluation metrics are used to measure the quality of the machine learning model, quantifies the performance of a predictive model.">
    <meta property="og:type" content="article">
    <meta property="og:image" content="">
    <meta name="twitter:card" content="summary">
    <meta name="description" content="Evaluation metrics are used to measure the quality of the machine learning model, quantifies the performance of a predictive model.">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicon-32x32.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.4.1/css/bootstrap.min.css">
    <link rel="manifest" href="manifest.json">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bitter">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Crete+Round">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="assets/fonts/fontawesome5-overrides.min.css">
    <link rel="stylesheet" href="assets/css/styles.min.css">
</head>

<body>
    <nav class="navbar navbar-light navbar-expand-md sticky-top" id="navigation" style="padding: 25px 0px;background-color: white;">
        <div class="container-fluid container"><a class="navbar-brand" href="https://devincept.codes/" style="background-image: url(&quot;assets/img/CODES.gif&quot;);background-position: center;background-size: cover;background-repeat: no-repeat;width: 170px;height: 65px;"></a><button data-toggle="collapse" class="navbar-toggler"
                data-target="#navcol-1"><span class="sr-only">Toggle navigation</span><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse text-uppercase" id="navcol-1" style="font-family: Roboto, sans-serif;font-weight: bold;">
                <ul class="nav navbar-nav ml-auto" style="font-size: 11px;">
                  <li class="nav-item" role="presentation"><a class="nav-link active"
                        href="https://devincept.codes/contribute.html">Contribute</a></li>
                  <li class="nav-item" role="presentation"><a class="nav-link active" href="https://devincept.tech/pricing.html">Free
                        courses</a></li>
                  <li class="nav-item" role="presentation"><a class="nav-link" href="https://devincept.codes/sponsor.html">Sponsor us</a>
                  </li>
                </ul>
            </div>
        </div>
    </nav>
    <section id="hero" style="background-color: #2f1b1b;color: rgb(255,255,255);">
        <div style="padding: 30px;">
            <p class="d-flex justify-content-center" style="font-family: Belgrano, serif;">&nbsp;Machine Learning</p>
            <h1 class="d-flex justify-content-center topic-title" style="font-family: Belgrano, serif;"><br><strong>Evaluation Metric</strong><br></h1>
        </div>
        <div id="base" style="background-color: rgba(255,255,255,0.1);padding: 10px 0;font-family: Belgrano, serif;">
            <p class="justify-content-center"><i class="fa fa-align-center"></i>&nbsp;&nbsp;<span id="visits">0</span>&nbsp;Reads</p>
        </div>
    </section>
    <section id="sidetab" style="margin-bottom: 0px;">
        <div class="container-fluid">
            <div class="row content-row">
                <div class="col-3 author">
                    <div class="author-details"><div>
    <div class="author">
        
        <h2 class="author-title"> Author's Details </h2>
        
        <h3 class="author's-name"> Sharath Chandrika </h3>
      
        
        <ul>

            <li><a target="_blank" href="https://www.linkedin.com/in/sharath-chandrika-avvaru-715421160/"><i class="fab fa-linkedin"></i></a></li> 
        </ul>
    </div>
</div></div>
                </div>
                <div class="col-9 main-content">
                    <div class="content-inner"><div>
  <div class="blog-content">
        
 
   <h1 id="what-is-evaluation-metric-">What is Evaluation metric?</h1>
   <ul>
      <li>Evaluation metrics are used to measure the quality of the machine learning model, quantifies the performance of a
         predictive model.</li>
      <li>Selecting a model, the data preparation methods together are a search problem which guided by the evaluation
         metric. </li>
      <li>Experiments are performed with different models and the outcome of each experiment is quantified with a metric.
      </li>
      <li>This typically involves training a model on a dataset, using the model to make predictions on a test dataset,
         then comparing the predictions to the expected values in the test dataset.</li>
      <li>For classification problems, metrics involve comparing the expected class label to the predicted class label or
         interpreting the predicted probabilities for the class labels for the problem. </li>
      <li>Evaluation of machine learning models or algorithms is needed for any project. </li>
      <li>Standard metrics works on most of the problems, that is why they are widely adopted.But all metrics make
         assumptions about the problem or about what is important in the problem.</li>
      <li>Therefore, an evaluation metric must be chosen according to the type of dataset and the model built on the
         dataset, which makes choosing model evaluation metrics challenging. </li>
      <li>This challenge is made difficult when there is a skew in the class distribution. </li>
      <li>The reason for this is that many of the standard metrics become unreliable or even misleading when classes are
         imbalanced, or severely imbalanced. </li>
   </ul>
   <h1 id="why-is-this-useful-">Why is this Useful?</h1>
   <ul>
      <li>We need to use multiple evaluation metrics to evaluate your model, which is very important because a model may
         perform well using one measurement from one evaluation metric, but may perform poorly using another measurement
         from another evaluation metric.</li>
      <li>Evaluation metrics are critical in ensuring that model is operating correctly and optimally. There are many
         different types of evaluation metrics are available to test a model. </li>
   </ul>
   <h2 id="1-confusion-matrix">1. Confusion Matrix</h2>
   <ul type="square">
      <li> A confusion matrix is a table with four different combinations of predicted and actual values.</li>
      <li> This is a good way to visualize the different outputs and to calculate the Precision, Recall, Accuracy, and F-1
         Score as well as the AUC-ROC. First will discuss about TP, FP, FN, and TN in a confusion matrix. </li>
   </ul>
   
   <p><img src="https://user-images.githubusercontent.com/63340338/93612837-a5a76d80-f9ed-11ea-9c05-44d2a749a8a7.png"
         alt="confusion_matrix_1"></p>
   <ul type="square">
      <li> True Positives (TP) : The number of times our model predicted TRUE and the actual output was also TRUE. </li>
   
      <li> True Negatives (TN): The number of times our model predicted FALSE and the actual output was FALSE. </li>
   
      <li> False Positives (FP): The number of times our model predicted TRUE and the actual output was FALSE. This is
         known as a Type 1 Error.</li>
   
      <li> False Negatives (FN): The number of times our model predicted FALSE and the actual output was TRUE. This is
         known as a Type 2 Error.</li>
   </ul>
   
   <p> <img src="https://user-images.githubusercontent.com/63340338/93614473-d12b5780-f9ef-11ea-8255-0908121fb3e8.jpg"
         alt="Type-I-and-II-errors1-625x468"></p>
   <h4 id="to-understand-this-concept-in-a-better-way-let-us-see-a-small-story-">To understand this concept in a better
      way, let us see a small story:</h4>
   <p> A shepherd boy gets bored tending the town&#39;s flock. To have some fun, he cries out, &quot;Wolf&quot; even though
      there was no wolf is in sight. The villagers run to protect the flock, but then they get angry when they realize the
      boy was playing a joke on them. Boy repeats this many times. One night, the shepherd boy sees a real wolf approaching
      the flock and calls out, &quot;Wolf&quot; The villagers thought it is a joke again and decided to stay back in their
      houses. The hungry wolf turns the flock into lamb chops. </p>
   <p> <em>Let us consider the following definitions:</em></p>
   <h5 id="-wolf-is-a-positive-class-"><em>&quot;Wolf&quot; is a positive class.</em></h5>
   <h5 id="-no-wolf-is-a-negative-class-"><em>&quot;No wolf&quot; is a negative class.</em></h5>
   <p> <em>Story can be visualised as our &quot;wolf-prediction&quot; model using a 2x2 confusion matrix that depicts all
         four possible outcomes:</em>
      <img src="https://user-images.githubusercontent.com/63340338/93613683-d0de8c80-f9ee-11ea-876a-013c3ea4cdc4.jpeg"
         alt="story"></p>
   <h2 id="2-precision">2. Precision</h2>
   <ul type="disc">
      <li> What ratio of positive identifications was actually correct? Answer for this question is Precision. </li>
   
      <li>Precision can be described as the fraction of relevant instances among the retrieved instances.</li>
   </ul>
  <p> The formula is as follows:</p>
   
   <img src="https://user-images.githubusercontent.com/63340338/93614096-53ffe280-f9ef-11ea-9907-2498878c1c2f.png" alt="2">
   
   <em> <p> In the terms of confusion matrix, the equation can be represented as:</p>
   
      <img src="https://user-images.githubusercontent.com/63340338/93614161-6da12a00-f9ef-11ea-960c-9e4df0e2bdf4.png"
         alt="1">
   
   </em> <p> Precision expresses the proportion of the data points our model says correct and actually were correct.</p>
   
   <h2> 3. Recall</h2>
   <ul type="square">
      <li> This answers the question “What proportion of actual positives was classified correctly?”</li>
      <li>Also known as sensitivity. </li>
   </ul>
<p>   This can be represented by the following equation:</p>
   
   <img src="https://user-images.githubusercontent.com/63340338/93614245-7abe1900-f9ef-11ea-8c13-33fd63cde835.png" alt="3">
   
 <p> In our confusion matrix, it would be represented by:</p>
   
   <img src="https://user-images.githubusercontent.com/63340338/93614274-8a3d6200-f9ef-11ea-9659-1820fcaa8138.png" alt="4">
   
   <ul type="square">
      <li> Recall expresses which instances are relevant in a data set. </li>
      <li> It is important to examine both the Precision AND Recall when evaluating a model because they often have an
         inverse relationship.</li>
      <li> When precision increases, recall tends to decrease and vice versa.</li>
   </ul>
   
   <h2 id="4-accuracy">4. Accuracy</h2>
   <ul>
      <li>Accuracy is determining out of all the classifications, how many did model classify correctly.This can be
         represented mathematically as: </li>
   </ul>
   <p><img src="https://user-images.githubusercontent.com/63340338/93614316-988b7e00-f9ef-11ea-918a-e5076fa38b69.png"
         alt="5"></p>
   <ul>
      <li>Using our confusion matrix terms, this equation is written as: </li>
   </ul>
   <p><img src="https://user-images.githubusercontent.com/63340338/93614355-a6d99a00-f9ef-11ea-9ca7-33d2e9162ca8.png"
         alt="6"></p>
   <ul>
      <li>We want the accuracy score to be as high as possible. </li>
      <li>It is important to note that accuracy may not always be the best metric to use, especially in cases of a
         class-imbalanced data set. This is when the distribution of data is not equal across all classes. </li>
      <li>For example, say that we are looking to build a model to help diagnose people with lung cancer. If our model just
         classified every single person as not having lung cancer, we would be correct the overwhelming majority of the
         time, but the cost of someone not being diagnosed when they do, in fact, have brain cancer is devastating.
         Depending on the industry, these costs may outweigh the accuracy of the model. In imbalanced cases like these, it
         is better to use the F1-Score instead. </li>
   </ul>
   <h2 id="5-f1-score">5. F1-Score</h2>
   <ul type="circle">
      <li> The F1 Score is a function of precision and recall.</li>
      <li> It is used to find the correct balance between the two metrics.</li>
      <li> It determines how many instances your model classifies correctly without missing a significant number of
         instances.</li>
      <li> This score can be represented by the following equation:</li>
   </ul>
   
   <p><img src="https://user-images.githubusercontent.com/63340338/93614395-b658e300-f9ef-11ea-82e1-d885ac8f71bf.png"
         alt="7"></p>
   <ul type="circle">
      <li> Having an imbalance between precision and recall, such as a high precision and low recall, can give you an
         extremely accurate model, but classifies difficult data incorrectly.</li>
      <li> We want the F1 Score to be as high as possible for the best performance of our model.</li>
   </ul>
   
   <h2 id="6-auc-area-under-curve-roc-receiver-operating-characteristics-curve">6. AUC (Area Under Curve) ROC(Receiver
      Operating Characteristics) Curve</h2>
   <ul>
      <li>The AUC ROC curve is a graph which shows the performance of a classification model at all thresholds.</li>
      <li>ROC is a probability curve and AUC represents degree of separability. ROC plots the following parameters: <ul>
            <li>True Positive Rate (TPR), known as recall or sensitivity, </li>
            <li>False Positive Rate (FPR), known as Fall-out, the ratio of the false positive predictions compared to all
               values that are actually negative. </li>
         </ul>
      </li>
   </ul>
   <p><img src="https://user-images.githubusercontent.com/63340338/93614571-f324da00-f9ef-11ea-84fa-bb8ca0a40a13.png"
         alt="9">
      <img src="https://user-images.githubusercontent.com/63340338/93614618-fe780580-f9ef-11ea-857b-3f7dcaa308d4.png"
         alt="10"></p>
   <ul>
      <li>Both the RPR and FPR are within the range [0, 1].</li>
      <li>The curve is the FPR vs TPR at different points in the range [0, 1]. </li>
      <li>The best performing classification models will have a curve similar to the green line in the graph below. </li>
      <li>The green line has the largest Area Under the Curve.</li>
      <li>The higher the AUC, the better your model is performing. </li>
      <li>A classifier with only 50–50 accuracy is realistically no better than randomly guessing, which makes the model
         worthless (red line). </li>
      <li>The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders.
         <img src="https://user-images.githubusercontent.com/63340338/93614669-0d5eb800-f9f0-11ea-8898-5075816dc8d9.png"
            alt="11"></li>
   </ul>
  

 <p class="ipynb-link">You can learn more about this topic from<a href="https://github.com/Learn-Write-Repeat/ml/blob/main/Evaluation%20Metric/SharathChandrika_ML_Evaluationmetric.ipynb">here</a>.</p>
 
   </div>


</div></div>
                </div>
            </div>
        </div>
    </section>
    <footer class="d-flex flex-column justify-content-center align-items-center" id="footer" style="padding-bottom: 0;">
        <h1>DevIncept.codes</h1>
        <h5>Contact us:</h5>
        <p>Email:<a href="#">&nbsp;support@devincept.tech</a></p>
    
        <div class="d-flex">
            <a href="https://www.linkedin.com/company/devincept/" class="btn btn-primary" role="button" aria-pressed="true"><i
                    class="fa fa-linkedin-square"></i></a>
            <a href="https://www.facebook.com/DevIncept" class="btn btn-primary" role="button" aria-pressed="true"><i
                    class="fa fa-facebook"></i></a>
            <a href="https://www.instagram.com/devincept.tech/" class="btn btn-primary" role="button" aria-pressed="true"><i
                    class="fa fa-instagram"></i></a>
            <a href="https://github.com/DevIncept" class="btn btn-primary" role="button" aria-pressed="true"><i
                    class="fa fa-github"></i></a>
        </div>   
    </footer>
    <script>
      function cb(response) {
         document.getElementById('visits').innerText = response.value;
      }
   </script>
   <script async src="https://api.countapi.xyz/hit/ml-evaluation-metric/visits?callback=cb"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
</body>

</html>
